{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educated-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.preprocessing as pre\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import harness\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brazilian-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternate-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_train = pd.read_csv('data/flights_train.csv', index_col=0)\n",
    "flights_test = pd.read_csv('data/flights_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nuclear-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_0 = pd.read_csv('weather_0.csv', index_col=0)\n",
    "weather_1 = pd.read_csv('weather_1.csv', index_col=0)\n",
    "weather_2 = pd.read_csv('weather_2.csv', index_col=0)\n",
    "weather_3 = pd.read_csv('weather_3.csv', index_col=0)\n",
    "weather_4 = pd.read_csv('weather_4.csv', index_col=0)\n",
    "weather = pd.concat([weather_0,weather_1,weather_2,weather_3,weather_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "united-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_test_clean = harness.clean_train(flights_test)\n",
    "flights_test_clean_copy = flights_test_clean.copy()\n",
    "flights_test_clean_copy['month']=flights_test_clean_copy.fl_date\n",
    "flights_test_clean_copy.month = flights_test_clean_copy.month.map(lambda v: int(v[5:7]))\n",
    "flights_test_clean_copy['day']=flights_test_clean_copy.fl_date\n",
    "flights_test_clean_copy.day = flights_test_clean_copy.day.map(lambda v: int(v[8:]))\n",
    "flights_test_clean_copy['haul']=flights_test_clean_copy.crs_elapsed_time/60\n",
    "flights_test_clean_copy['haul'] = pd.cut(flights_test_clean_copy.haul,bins=[0,3,6,12],labels=['Short','Medium','Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lesbian-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean = harness.clean_train(flights_train)\n",
    "flights_clean_copy = flights_clean.copy()\n",
    "flights_clean_copy['month']=flights_clean_copy.fl_date\n",
    "flights_clean_copy.month = flights_clean_copy.month.map(lambda v: int(v[5:7]))\n",
    "flights_clean_copy['day']=flights_clean_copy.fl_date\n",
    "flights_clean_copy.day = flights_clean_copy.day.map(lambda v: int(v[8:]))\n",
    "flights_clean_copy['haul']=flights_clean_copy.crs_elapsed_time/60\n",
    "flights_clean_copy['haul'] = pd.cut(flights_clean_copy.haul,bins=[0,3,6,12],labels=['Short','Medium','Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'op_unique_carrier')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'fl_date')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'dest_city_name')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'origin_airport_id')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'origin_city_name')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'dest_airport_id')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'day')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'month')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'haul')\n",
    "flights_test_clean_copy = harness.add_grouped_stats(flights_test_clean_copy,'tail_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "postal-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'op_unique_carrier')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'fl_date')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'dest_city_name')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'origin_airport_id')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'origin_city_name')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'dest_airport_id')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'day')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'month')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'haul')\n",
    "flights_clean_copy = harness.add_grouped_stats(flights_clean_copy,'tail_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mounted-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_test_clean_copy = flights_test_clean_copy.drop(['dup',\n",
    "                                                  'op_unique_carrier', \n",
    "                                                  'fl_date','dest_city_name',\n",
    "                                                  'origin_airport_id','origin_city_name',\n",
    "                                                  'dest_airport_id','day','month','haul',\n",
    "                                                  'branded_code_share',\n",
    "                                                  'mkt_carrier','origin','dest',\n",
    "                                                  'tail_num','mkt_unique_carrier'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "anticipated-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean_copy = flights_clean_copy.drop(['dup',\n",
    "                                                  'op_unique_carrier', \n",
    "                                                  'fl_date','dest_city_name',\n",
    "                                                  'origin_airport_id','origin_city_name',\n",
    "                                                  'dest_airport_id','day','month','haul',\n",
    "                                                  'branded_code_share',\n",
    "                                                  'mkt_carrier','origin','dest',\n",
    "                                                  'tail_num','mkt_unique_carrier'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "particular-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_test_clean_copy['tail_num_delay_std'] = flights_test_clean_copy['tail_num_delay_std'].fillna(0)\n",
    "flights_test_clean_copy['dest_airport_id_delay_std'] = flights_test_clean_copy['dest_airport_id_delay_std'].fillna(0)\n",
    "flights_test_clean_copy['origin_city_name_delay_std'] = flights_test_clean_copy['origin_city_name_delay_std'].fillna(0)\n",
    "flights_test_clean_copy['origin_airport_id_delay_std'] = flights_test_clean_copy['origin_airport_id_delay_std'].fillna(0)\n",
    "flights_test_clean_copy['dest_city_name_delay_std'] = flights_test_clean_copy['dest_city_name_delay_std'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "corresponding-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_clean_copy['tail_num_delay_std'] = flights_clean_copy['tail_num_delay_std'].fillna(0)\n",
    "flights_clean_copy['dest_airport_id_delay_std'] = flights_clean_copy['dest_airport_id_delay_std'].fillna(0)\n",
    "flights_clean_copy['origin_city_name_delay_std'] = flights_clean_copy['origin_city_name_delay_std'].fillna(0)\n",
    "flights_clean_copy['origin_airport_id_delay_std'] = flights_clean_copy['origin_airport_id_delay_std'].fillna(0)\n",
    "flights_clean_copy['dest_city_name_delay_std'] = flights_clean_copy['dest_city_name_delay_std'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alternative-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = flights_clean_copy.drop('arr_delay',axis=1)\n",
    "y_train = flights_clean_copy.arr_delay\n",
    "X_test = flights_test_clean_copy.drop('arr_delay',axis=1)\n",
    "y_test = flights_test_clean_copy.arr_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spatial-plastic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RFR = RandomForestRegressor(n_estimators=10)\n",
    "RFR = RFR.fit(X_train,y_train)\n",
    "y_pred = RFR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cubic-naples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755110488044271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-warrior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
